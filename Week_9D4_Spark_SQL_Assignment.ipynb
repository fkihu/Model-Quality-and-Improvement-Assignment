{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 9D4 Spark SQL Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjWVuwmHyxzwWTCeQny/oN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fkihu/Model-Quality-and-Improvement-Assignment/blob/main/Week_9D4_Spark_SQL_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "As a Data professional, you need to perform an analysis by answering questions about\n",
        "some stock market data on Safaricom from the years 2012-2017. Data source: https://bit.ly/3pmchka\n",
        "\n",
        "This analysis will be done on PySpark"
      ],
      "metadata": {
        "id": "CDdNQ3eO-yzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data importation and Exploration"
      ],
      "metadata": {
        "id": "KuXl2ypq_3vR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjpwqfE8-WV2",
        "outputId": "c14c9f00-abff-4184-cf61-9aaa2b5c86fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ],
      "source": [
        "# Starting the spark session and loading the stock file \n",
        "\n",
        "# Installing pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Running a local Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Downloading the dataset\n",
        "from pyspark.sql import SQLContext\n",
        "sqlCtx = SQLContext(sc)\n",
        "df = sqlCtx.read.csv(\"saf_stock.csv\")\n",
        "df.createOrReplaceTempView('saf_stock')\n",
        "tables = sqlCtx.tableNames()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdiGy8hCBLry",
        "outputId": "ee08bfb9-f73f-4692-96ce-361b6efad763"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['saf_stock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = sqlCtx.read.csv(\"saf_stock.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-rlLz-bYh5",
        "outputId": "44e0e4ea-3496-4d4a-8509-4c129b79d332"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "# display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrIMmsEdYkQm",
        "outputId": "635652c7-471d-4df2-c3e1-cb204bd0e33a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "1. The schema did not automatically pick the labels in the dataframe.\n",
        "2. The column names are 'c0', 'c1', 'c2', 'c3', 'c4', 'c5' and 'c6'"
      ],
      "metadata": {
        "id": "oM75Ahy59NjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferring the schema and setting the column headings\n",
        "\n",
        "# File location and type\n",
        "file_location = \"saf_stock.csv\"\n",
        "file_type = \"csv\"\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df2 = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "# Displaying the Dataframe\n",
        "display(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ke9pmZ_LwN3j",
        "outputId": "fd973f35-86d2-4972-ee84-94ec88d5fd66"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Date: string, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe now has the correct column names."
      ],
      "metadata": {
        "id": "FxETj3VZyAlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Dataframe's Schema\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUPibCHVw6rj",
        "outputId": "b8e6a21c-99b1-4b94-819b-40de2d722606"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the first five rows of the dataframe\n",
        "df2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWinAJY0WIPS",
        "outputId": "9bb9b242-43f5-4962-865d-266aa7ccc777"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the statistic summary of the data.\n",
        "\n",
        "df2.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWCV2Iraye2K",
        "outputId": "9f317070-d30a-425a-b1a9-f5a071fd59e8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "ee4OKx8pqxdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format all the data to 2 decimal places i.e. format_number()\n",
        "\n",
        "# from pyspark.sql.functions import avg, format_number \n",
        "\n",
        "# Importing format_number\n",
        "from pyspark.sql.functions import format_number, col\n",
        "\n",
        "# Selecting the columns to format\n",
        "cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "\n",
        "# Formatting the columns\n",
        "df2 = df2.select('Date', *[format_number(col(col_name), 2).name(col_name) for col_name in cols]).show()\n",
        "# df2.describe().show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8HKpvi-q1nA",
        "outputId": "e919dce1-3704-4017-ba08-fbacc753b23d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "|      Date| Open| High|  Low|Close|       Volume|Adj Close|\n",
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12,668,800.00|    52.62|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9,593,300.00|    52.08|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12,768,200.00|    51.83|\n",
            "|2012-01-06|59.42|59.45|58.87|59.00| 8,069,400.00|    51.46|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6,679,300.00|    51.62|\n",
            "|2012-01-10|59.43|59.71|58.98|59.04| 6,907,300.00|    51.49|\n",
            "|2012-01-11|59.06|59.53|59.04|59.40| 6,365,600.00|    51.81|\n",
            "|2012-01-12|59.79|60.00|59.40|59.50| 7,236,400.00|    51.90|\n",
            "|2012-01-13|59.18|59.61|59.01|59.54| 7,729,300.00|    51.93|\n",
            "|2012-01-17|59.87|60.11|59.52|59.85| 8,500,000.00|    52.20|\n",
            "|2012-01-18|59.79|60.03|59.65|60.01| 5,911,400.00|    52.34|\n",
            "|2012-01-19|59.93|60.73|59.75|60.61| 9,234,600.00|    52.86|\n",
            "|2012-01-20|60.75|61.25|60.67|61.01|10,378,800.00|    53.21|\n",
            "|2012-01-23|60.81|60.98|60.51|60.91| 7,134,100.00|    53.13|\n",
            "|2012-01-24|60.75|62.00|60.75|61.39| 7,362,800.00|    53.54|\n",
            "|2012-01-25|61.18|61.61|61.04|61.47| 5,915,800.00|    53.61|\n",
            "|2012-01-26|61.80|61.84|60.77|60.97| 7,436,200.00|    53.18|\n",
            "|2012-01-27|60.86|61.12|60.54|60.71| 6,287,300.00|    52.95|\n",
            "|2012-01-30|60.47|61.32|60.35|61.30| 7,636,900.00|    53.47|\n",
            "|2012-01-31|61.53|61.57|60.58|61.36| 9,761,500.00|    53.52|\n",
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new data frame with a column called HV Ratio that is the ratio of the\n",
        "# High Price versus volume of stock traded for a day\n",
        "\n",
        "# TO REVISIT THIS CODE AS IT GENERATED AN ERROR\n",
        "\n",
        "# new_df = df2.withColumn('HV Ratio', df2.High/df2.Volume).select('High', 'Volume', 'HV Ratio').show()"
      ],
      "metadata": {
        "id": "cagntTG41Xlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "G94hbWk5_4Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What day had the Peak High in Price?\n",
        "from pyspark.sql.functions import desc\n",
        "df2.select('Date', 'High').orderBy(desc('High')).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbTsGtNkAzba",
        "outputId": "13f4cc69-f01f-45d5-95ef-ba24089625d6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|      Date|     High|\n",
            "+----------+---------+\n",
            "|2015-01-13|90.970001|\n",
            "+----------+---------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The day that had the peak high in price was 13th January 2015."
      ],
      "metadata": {
        "id": "xVMhY5NBC0rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the mean of the Close column?\n",
        "df2.select('Close').describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAAzx25EEuyM",
        "outputId": "d2afc149-4518-4c7f-ede2-036a3906de2b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|summary|            Close|\n",
            "+-------+-----------------+\n",
            "|  count|             1258|\n",
            "|   mean|72.38844998012726|\n",
            "| stddev|6.756859163732991|\n",
            "|    min|        56.419998|\n",
            "|    max|        90.470001|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The mean of the Close column is 72.38844998012726"
      ],
      "metadata": {
        "id": "u-NmJw8qFY6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the max and min of the Volume column?\n",
        "df2.select('Volume').describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8m2VV2EFfLs",
        "outputId": "45d0b74e-6c77-4047-bd91-b4fc4d757b32"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|summary|           Volume|\n",
            "+-------+-----------------+\n",
            "|  count|             1258|\n",
            "|   mean|8222093.481717011|\n",
            "| stddev|  4519780.8431556|\n",
            "|    min|          2094900|\n",
            "|    max|         80898100|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The min and max of the Volume column is 2094900 and 80898100 respectively."
      ],
      "metadata": {
        "id": "aynq2TETFlHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many days was the Close lower than 60 dollars?\n",
        "df2.filter((df2['Close'] < 60)).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AJPvy7SF-qM",
        "outputId": "8f6018f5-315d-4339-a8a8-a13e1d806c25"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: There were 81 days where the Close was less than 60 dollars."
      ],
      "metadata": {
        "id": "ej2lrW_iHpwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What percentage of the time was the High greater than 80 dollars?\n",
        "\n",
        "# Getting the count of the days when the High was greater than 80 dollars.\n",
        "df2.filter((df2['High'] > 80)).count() #115 Days\n",
        "\n",
        "# There are a total of 1258 records in the dataframe.\n",
        "\n",
        "# Computing the percentage of the time when High was greater than 80 dollars.\n",
        "\n",
        "answer = 115/1258\n",
        "print(format(answer,'.2%')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOIHk8G-H4pR",
        "outputId": "6ef4cedd-734c-41a5-b26e-c63f7614d37a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: the percentage of the time that the High was greater than 80 dollars was 9.14%."
      ],
      "metadata": {
        "id": "4zB67Jz3JvGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the Pearson correlation between High and Volume?\n",
        "df2.select('High', 'Volume').corr('High', 'Volume')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r3h7R3_J51N",
        "outputId": "0b3539f8-04e3-4812-a28a-a04312a3d48f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3384326061737161"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The correlation coefficient between High and Volume is -0.3384326061737161"
      ],
      "metadata": {
        "id": "asg_AglvLTZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the max High per year?\n",
        "\n",
        "# Import relevant libraries\n",
        "from pyspark.sql.functions import dayofmonth,hour,dayofyear,weekofyear,month,year,format_number,date_format,mean, date_format, datediff, to_date, lit\n",
        "\n",
        "# Adding the Year column\n",
        "new_df2 = df2.withColumn('Year', year(df2['Date']))\n",
        "\n",
        "# Showing the max High per year\n",
        "new_df2.groupBy('Year').max('High').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YEwJ98ILmSV",
        "outputId": "848a4618-a7c0-4baf-fc71-108ace89235c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: the year with the highest max(High) was 2015 while 2016 had the lowest."
      ],
      "metadata": {
        "id": "z1LxOu1_R_3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the average Close for each Calendar Month?\n",
        "\n",
        "# Adding the Month column\n",
        "new_df2 = df2.withColumn('Month', month(df2['Date']))\n",
        "\n",
        "# Showing the average Close per month\n",
        "from pyspark.sql.functions import asc\n",
        "new_df2.groupBy('Month').mean('Close').orderBy(asc('Month')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPdB2lFvR9zW",
        "outputId": "bad2086d-be7e-4307-ab87-8986944ae623"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|    1|71.44801958415842|\n",
            "|    2|  71.306804443299|\n",
            "|    3|71.77794377570092|\n",
            "|    4|72.97361900952382|\n",
            "|    5|72.30971688679247|\n",
            "|    6| 72.4953774245283|\n",
            "|    7|74.43971943925233|\n",
            "|    8|73.02981855454546|\n",
            "|    9|72.18411785294116|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|   12|72.84792478301885|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The month with the highest closing average is July (74.43971943925233), while February has the lowest (71.306804443299)."
      ],
      "metadata": {
        "id": "fcdF-SbyUDJ1"
      }
    }
  ]
}